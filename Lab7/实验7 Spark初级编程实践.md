# 实验目的
- 通过使用 Spark 访问本地文件和 HDFS 文件的方法。
- 掌握 Spark 应用程序的编写、编译和运行方法。

# 实验平台
- 操作系统：Ubuntu 22.04
- JDK 版本：JDK 1.8.0_162
- Hadoop 版本：3.3.5
- Spark 版本：3.4.0

# 实验内容和要求
### 一、安装 Hadoop 和 Spark

完成 Hadoop 伪分布式模式的安装后，进行 Spark（Local 模式）的安装。

### 二、Spark 读取文件系统的数据

##### 1. 使用 Spark Shell 读取本地文件

在 spark-shell 中读取本地文件 /tmp/test.txt 并统计出文件的行数

##### 2. 使用 Spark Shell 读取 HDFS 文件

在 Spark-shell 中读取 HDFS 文件 /test/test.txt 并统计出文件的行数

##### 3. 编程读取 HDFS 文件

编写独立应用程序，读取 HDFS 文件 /user/hadoop/test.txt ，然后统计出文件的行数；通过 sbt 将整个应用程序编译打包成 Jar 包，并将生成的 Jar 包通过 spark-submit 提交到 Spark 中运行。

### 三、编写 Spark 独立应用程序实现数据去重

对于两个输入文件 A 和 B，编写 Spark 独立应用程序，对两个文件进行合并，并剔除其中重复的内容，得到一个新文件 C。下面是输入文件和输出文件的一个样例，可供参考。

输入文件 A 的样例如下：

```
20210501 x
20210502 y
20210503 x
20210504 y
20210505 z
20210506 x
```

输入文件 B 的样例如下：

```
20210501 y
20210502 y
20210503 x
20210504 z
20210505 y
```

根据输入文件 A 和 B 合并得到的输出文件 C 的样例如下：

```
20210501 x
20210501 y
20210502 y
20210503 x
20210504 y
20210504 z
20210505 z
20210505 y
20210506 x
```

### 四、编写 Spark 独立应用程序实现求平均值

每个输入文件表示班级学生某个学科的成绩，每行内容由两个字段组成，第一个是学生名字，第二个是学生的成绩；编写 Spark 独立应用程序求出所有学生的平均成绩，并输出到一个新文件中。下面是输入文件和输出文件的一个样例，可供参考。

Algorithm 成绩：

```
小明 92
小红 87
小新 82
小丽 90
```

Database 成绩：

```
小明 95
小红 81
小新 89
小丽 85
```

Python 成绩：

```
小明 82
小红 83
小新 94
小丽 91
```

平均成绩如下：

```
小红 83.67
小新 88.33
小明 89.67
小丽 88.67
```